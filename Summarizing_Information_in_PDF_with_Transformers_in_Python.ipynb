{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMY2RR5Z4KGH3r+6I9dEznc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haddjyb2k/-APPLIED-DATA-SECIENCE-CAPSTONE/blob/main/Summarizing_Information_in_PDF_with_Transformers_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarizing Information in PDF with Transformers in Python"
      ],
      "metadata": {
        "id": "guGPjRoTzawm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzwEBw5fzfU6",
        "outputId": "5bb9c061-c8d7-460d-9936-8daf04b55516"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.12.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFDtl6jb0hdh",
        "outputId": "b697a746-f418-4a5a-b9f6-ba28b89d02c7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.12.0\n",
            "  Using cached transformers-4.12.0-py3-none-any.whl (3.1 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.12.0) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.12.0) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.12.0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.12.0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.12.0) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.12.0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.12.0) (2.27.1)\n",
            "Collecting sacremoses (from transformers==4.12.0)\n",
            "  Using cached sacremoses-0.0.53-py3-none-any.whl\n",
            "Collecting tokenizers<0.11,>=0.10.1 (from transformers==4.12.0)\n",
            "  Using cached tokenizers-0.10.3.tar.gz (212 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.12.0) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.17->transformers==4.12.0) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.17->transformers==4.12.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.12.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.12.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.12.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.12.0) (3.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.12.0) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.12.0) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.12.0) (1.2.0)\n",
            "Building wheels for collected packages: tokenizers\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build tokenizers\n",
            "\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preparation & Collection"
      ],
      "metadata": {
        "id": "Qb4WMAIk1Ed2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAUuhWQP2grE",
        "outputId": "c0419d11-d1ef-408d-81ba-6d3e2fb04635"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (8.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "\n",
        "# Directory for storing PDF resumes and job applications\n",
        "pdf_directory = '/content/pdf_files'\n",
        "\n",
        "# Directory for storing extracted text from PDFs\n",
        "text_directory = '/content/extracted_text'\n",
        "\n",
        "# OCR output directory for scanned PDFs\n",
        "ocr_directory = '/content/ocr_output'\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(pdf_directory, exist_ok=True)\n",
        "os.makedirs(text_directory, exist_ok=True)\n",
        "os.makedirs(ocr_directory, exist_ok=True)"
      ],
      "metadata": {
        "id": "QOaZoUiO1FhI"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Organizing the PDFs\n",
        "Create a coherent folder structure to organize the PDF files systematically. Utilize appropriate categorization methods such as job positions, application dates, or candidate names to ensure a logical arrangement of the files. This organizational framework facilitates easy retrieval and enhances data handling efficiency throughout the project."
      ],
      "metadata": {
        "id": "32WmKGIG3L9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PDF Format Handling and Text Extraction\n",
        "PDF files often exhibit diverse formats, layouts, and encodings. Account for these variations by employing appropriate preprocessing techniques. In the provided code snippet, the PyPDF2 library is utilized to open each PDF file, extract text from each page, and save the extracted text as individual text files. The extracted text is stored in the ‘/content/extracted_text.’ directory. This step standardizes the data and ensures that the text content is readily accessible for further processing stages.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OynHQU2x3aOq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZbG1CiFp3bh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for file_name in os.listdir(pdf_directory):\n",
        "    if file_name.endswith('.pdf'):\n",
        "        # Open the PDF file\n",
        "        with open(os.path.join(pdf_directory,'JD Head Works and General Services.pdf'), 'rb') as file:\n",
        "            # Create a PDF reader object\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "            # Extract text from each page\n",
        "            text = ''\n",
        "            for page in reader.pages:\n",
        "                text += page.extract_text()\n",
        "\n",
        "            # Save the extracted text as a text file\n",
        "            text_file_name = file_name.replace('.pdf', '.txt')\n",
        "            text_file_path = os.path.join(text_directory, text_file_name)\n",
        "            with open(text_file_path, 'w') as text_file:\n",
        "                text_file.write(text)"
      ],
      "metadata": {
        "id": "CQ0ffIHz3M4L"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OCR for Scanned PDFs\n",
        "Scanned PDFs or PDFs containing images require Optical Character Recognition (OCR) techniques to convert the embedded images into machine-readable text. The code snippet showcases the utilization of the pytesseract library to perform OCR on scanned PDFs. The OCR text is saved as separate files in the ‘/content/ocr_output’ directory. This optional step unlocks the text content embedded within scanned PDFs, broadening the scope of data processing.\n",
        "\n"
      ],
      "metadata": {
        "id": "P2DR-xdT3-IJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional Step\n",
        "for file_name in os.listdir(pdf_directory):\n",
        "    if file_name.endswith('.pdf'):\n",
        "        # Open the PDF file\n",
        "        with Image.open(os.path.join(pdf_directory,'JD Head Works and General Services.pdf')) as img:\n",
        "            # Perform OCR using pytesseract\n",
        "            ocr_text = pytesseract.image_to_string(img, lang='eng')\n",
        "\n",
        "            # Save the OCR output as a text file\n",
        "            ocr_file_name = file_name.replace('.pdf', '.txt')\n",
        "            ocr_file_path = os.path.join(ocr_directory, ocr_file_name)\n",
        "            with open(ocr_file_path, 'w') as ocr_file:\n",
        "                ocr_file.write(ocr_text)"
      ],
      "metadata": {
        "id": "fcn7cAqU0hg-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "41d984bb-195b-4985-8a8a-71bf472b6b87"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnidentifiedImageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-d06113fe4faa>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# Open the PDF file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_directory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'JD Head Works and General Services.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0;31m# Perform OCR using pytesseract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mocr_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytesseract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'eng'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3028\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maccept_warnings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3029\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3030\u001b[0;31m     raise UnidentifiedImageError(\n\u001b[0m\u001b[1;32m   3031\u001b[0m         \u001b[0;34m\"cannot identify image file %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3032\u001b[0m     )\n",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file '/content/pdf_files/JD Head Works and General Services.pdf'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PDF Parsing and Text Extraction\n",
        "\n",
        "o access the valuable information within PDF resumes and job applications, it is crucial to parse the\n",
        "PDF files and extract the text content. This process involves addressing various formats, layouts, and challenges that may arise. Let’s delve into the steps required for parsing and extracting text from PDF files:\n",
        "\n",
        "Set up the directory where the PDF resumes and job applications are stored. In this case, we utilize the ‘/content/pdf_files’ directory as the designated location.\n",
        "Obtain a list of files present in the specified PDF directory. Filter out only the PDF files by examining their extensions and considering those ending with ‘.pdf.’\n",
        "Employ a loop to iterate over each resume file. For every file, follow the subsequent procedures:\n",
        "A. Opening the File: Open the resume file in ‘rb’ (read binary) mode using the open() function and a context manager. This ensures secure file handling and automatic closure upon completion.\n",
        "\n",
        "B. Creating a PDF Reader Object:  To establish a PDF reader object, use the PyPDF2 library’s PdfReader() functiont. This object enables access to the content within the PDF file.\n",
        "\n",
        "C. Extracting Text from Pages: Extract the text content from each PDF file page. Employ a loop to\n",
        "iterate through the pages using the pages attribute of the PDF reader object. Extract the text from each page using the extract_text() method and concatenate it with the existing text.\n",
        "\n",
        "D. The extracted text within the text variable is accumulated throughout the extraction process. This variable holds the combined text content derived from all pages within the PDF file.\n",
        "\n"
      ],
      "metadata": {
        "id": "TwDeqKXk4-JJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory for storing PDF resumes and job applications\n",
        "pdf_directory = '/content/pdf_files'\n",
        "\n",
        "resume_files = []\n",
        "for file_name in os.listdir(pdf_directory):\n",
        "    if file_name.endswith('.pdf'):\n",
        "        resume_files.append(os.path.join(pdf_directory, file_name))\n",
        "\n",
        "resume_summaries = []  # To store the generated summaries\n",
        "\n",
        "# Loop through each resume file\n",
        "for resume_file in resume_files:\n",
        "    with open(resume_file, 'rb') as file:\n",
        "        # Create a PDF reader object\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "        # Extract text from each page\n",
        "        def reader_pages(text):\n",
        "          text = ''\n",
        "          for page in reader_pages:\n",
        "            text += page.extract_text()\n",
        ""
      ],
      "metadata": {
        "id": "a9etT2zM7_q5"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing Text Summarization with Transformers\n",
        "In the pursuit of text summarization, transformers have emerged as cutting-edge deep learning architecture. They exhibit exceptional capabilities in condensing information while retaining the essence of the original text. Let’s dive into the implementation steps, highlighting the utilization of pre-trained models like T5 for text summarization.\n",
        "\n",
        "Model and Tokenizer Initialization: Begin by initializing the T5 model and tokenizer. These components serve as the backbone of our text summarization pipeline. In this instance, we instantiate the T5 model with the “t5-base” architecture.\n",
        "Text Encoding: Prepare the input text for summarization by encoding it using the tokenizer. This step converts the text into a numerical representation that the model can understand. To guide the model towards summarization, we prepend the text with the instruction “summarize.”\n",
        "Summary Generation: Leverage the power of the model to generate a summary. Employing a beam search algorithm, the model explores various paths to produce the most fitting summary. Fine-tune the summary length, quality, and other parameters, such as length penalty and the number of beams, to achieve desired outcomes.\n",
        "Summary Decoding: Using the tokenizer, decode the summary’s numerical representation into human-readable text. This decoding step allows us to obtain a comprehensive overview that encapsulates the crucial details from the original text.\n",
        "Storing the Summaries: Capture the generated summaries by keeping them in the resume_summaries list, providing a centralized repository for future utilization.\n",
        "Printing the Summaries: Iterate through the resume_summaries list and present the generated summaries for each resume, accompanied by an appropriate identifier."
      ],
      "metadata": {
        "id": "M-Phm60c8Ns4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#t5_sp_model_path = \"https://download.pytorch.org/models/text/t5_tokenizer_base.model\"\n",
        "\n",
        "#model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")"
      ],
      "metadata": {
        "id": "HWEiwV8MGpDW"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hpFW6WRgC7p",
        "outputId": "9b660746-a6e4-494e-bf3b-e04fd51cefea"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from transformers import T5Tokenizer,T5ForConditionalGeneration,Adafactor\n",
        "#!pip install text\n",
        "#tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CghTpl1jeElL",
        "outputId": "531c7841-600e-45c2-b215-18cd14a73aac"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement text (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for text\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Continuing the loop from the previous step\n",
        "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
        "\n",
        "# Initialize the model and tokenizer\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "#model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "\n",
        "\n",
        "        # Encode the text\n",
        "inputs = tokenizer.encode(\"summarize: \" + text,return_tensors=\"pt\", max_length=1000, truncation=True)\n",
        "\n",
        "        # Generate the summary\n",
        "outputs = model.generate(inputs,\n",
        "        max_length=1000, min_length=100,\n",
        "        length_penalty=2.0, num_beams=4,\n",
        "        early_stopping=True)\n",
        "\n",
        "        # Decode the summary\n",
        "summary = tokenizer.decode(outputs[0])\n",
        "\n",
        "resume_summaries.append(summary)\n",
        "\n",
        "# Print the generated summaries for each resume\n",
        "for i, summary in enumerate(resume_summaries):\n",
        "  print(f\"Summary for Resume{i+1}:\")\n",
        "  print(summary)\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLFJXDge8WX3",
        "outputId": "ca5ce527-a88b-4937-e74f-ac5aca0a913c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary for Resume1:\n",
            "<pad> head, works & general services is responsible for ensuring the plant has the most suitable working environment for its employees. responsibilities include: ensuring plant is litter free and when required, liaise with external contractors to maintain cost-effective schedule for waste disposal. responsibilities include: ensuring plant is litter free and ensuring the efficiency of the refrigeration & AC systems, heating and lighting systems throughout the premises. Ensure plant is litter free and when required, liaise with external contractors to maintain cost-effective schedule</s>\n",
            "\n"
          ]
        }
      ]
    }
  ]
}